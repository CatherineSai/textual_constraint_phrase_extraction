{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that a folder called \"models\" exists in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Method A: Create pipeline and save as a spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catherinesai/Documents/01_Lehre/Abschlussarbeiten/Abgabe_WS22/Elsa Fernanda/phrase_extraction-main/.elsa_fernanda_venv/lib64/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from phrase_extraction import *\n",
    "\n",
    "# Create and save model with phrase_spans component - only needs to run once\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "nlp.add_pipe('merge_entities')\n",
    "nlp.add_pipe('phrase_spans')\n",
    "nlp.to_disk('./models/method_a')\n",
    "\n",
    "# Afterwards, to use the above model with pipeline, simply load the previously saved model\n",
    "# nlp = spacy.load('models/method_a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Method B: Train and save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Use spaCy\n",
    "\n",
    "If you do not have Prodigy installed, then you can use our pre-annotated spaCy corpus in the `./dataset/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ./dataset/corpus_b/config.cfg --output ./models/method_b --paths.train ./dataset/corpus_b/train.spacy --paths.dev ./dataset/corpus_b/dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Use Prodigy\n",
    "\n",
    "If you have Prodigy license, train directly with Prodigy:\n",
    "\n",
    "In order to be able to train directly with Prodigy, annotation saved in the internal Prodigy database is needed. One possibility is to train your dataset manually, or import our pre-annotated dataset under `./dataset/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.A: Annotate using Prodigy manually:\n",
    "\n",
    "Run the command to start a prodigy web server and annotate the spans to save them in the specificed database (see [Prodigy docs](https://prodi.gy/docs/span-categorization#manual)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy spans.manual gold_standard blank:en ./dataset/gold_standard.jsonl --label SUBJECT,SIGNAL,VERB,TIME,CONDITION,OBJECT,OP_SUBJECT,OP_SIGNAL,OP_VERB,OP_TIME,OP_CONDITION,OP_OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy spans.manual training_data blank:en ./dataset/training_data.jsonl --label SUBJECT,SIGNAL,VERB,TIME,CONDITION,OBJECT,OP_SUBJECT,OP_SIGNAL,OP_VERB,OP_TIME,OP_CONDITION,OP_OBJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.B: Import the pre-annotated dataset into Prodigy:\n",
    "\n",
    "I have already annotated 3 datasets under `./dataset/` (`annotated_gold_standard.jsonl`, `annotated_reach_data.jsonl`, `annotated_training_data.jsonl`). You can simply load these files into the internal Prodigy database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy db-in gold_standard ./dataset/annotated_gold_standard.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy db-in training_data ./dataset/annotated_training_data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy db-in reach_data ./dataset/annotated_reach_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train a model from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy train ./models/method_b --spancat training_data,eval:gold_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Export Prodigy database to spaCy training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy data-to-spacy ./dataset/corpus_b --spancat training_data,eval:gold_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Method C: Train and save model\n",
    "\n",
    "For method C, since we are using a custom training configuration (`config.cfg` file), we must use spaCy to train our model. We also need to include the path to the python script containing our custom suggester function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ./dataset/corpus_c/config.cfg --output ./models/method_c --paths.train ./dataset/corpus_c/train.spacy --paths.dev ./dataset/corpus_c/dev.spacy --code span_suggester.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('prodigy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7c008156a7eef8425ceb7aafc21316cfc3e9ec861ed5228bde0cd1e797825fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
